{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "img_auto_encoders.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBFevDC+uzlw2F7QvEa+2s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjahan/semantic_similarity/blob/image_encoder/examples/colab/img_auto_encoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXuhobmin18J"
      },
      "source": [
        "## Image auto encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uejaf39rn8ko"
      },
      "source": [
        "## Install packages\n",
        "\n",
        "Ref: https://medium.com/analytics-vidhya/image-similarity-model-6b89a22e2f1a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU97rGfpoJn4"
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp8wObwPpD6t"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7GSaVotpEDh",
        "outputId": "21a2d2ff-c698-4817-f3cf-87e5613573d8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DxlneCqmuKy"
      },
      "source": [
        "DATA_PATH = '../content/gdrive/MyDrive/dataset/dataset'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbHX53_poQc9"
      },
      "source": [
        "## Split dataset to Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mACAJMJmoKFw",
        "outputId": "9f95bed8-b4d5-46cb-96d4-bb938b53a80c"
      },
      "source": [
        "# Extracting image paths\n",
        "file_path = os.listdir(DATA_PATH)\n",
        "print(len(file_path))\n",
        "\n",
        "train_files, test_files = train_test_split(file_path, test_size = 0.15)\n",
        "\n",
        "print(\"Number of Training Images:\",len(train_files))\n",
        "print(\"Number of Test Images: \",len(test_files))\n",
        "train_files = pd.DataFrame(train_files, columns=['filepath'])\n",
        "test_files = pd.DataFrame(test_files, columns=['filepath'])\n",
        "\n",
        "# converting into .csv file for future reference.\n",
        "train_files.to_csv('train_file.csv')\n",
        "test_files.to_csv('test_file.csv')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4788\n",
            "Number of Training Images: 4069\n",
            "Number of Test Images:  719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkSlg_o2lkOm"
      },
      "source": [
        "## Read images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Kqw_mZoVT6"
      },
      "source": [
        "def image2array(file_array, data_path):\n",
        "    \"\"\"\n",
        "    Reading and Converting images into numpy array by taking path of images.\n",
        "    Arguments:\n",
        "    file_array - (list) - list of file(path) names\n",
        "    Returns:\n",
        "    A numpy array of images. (np.ndarray)\n",
        "    \"\"\"\n",
        "\n",
        "    image_array = []\n",
        "    for fn in tqdm(file_array):\n",
        "        img = cv2.imread(data_path + '/' + fn)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (224,224))\n",
        "        image_array.append(np.array(img))\n",
        "\n",
        "    image_array = np.array(image_array)\n",
        "    image_array = image_array.reshape(image_array.shape[0], 224, 224, 3)\n",
        "    image_array = image_array.astype('float32')\n",
        "    image_array /= 255\n",
        "\n",
        "    return np.array(image_array)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj9arZKhlpPU",
        "outputId": "c0e75707-6e4c-47cc-a86b-819a18ac38c3"
      },
      "source": [
        "train_data = image2array(train_files['filepath'].tolist()[:100], DATA_PATH)\n",
        "print(\"Length of training dataset:\",train_data.shape)\n",
        "test_data = image2array(test_files['filepath'].tolist()[:10], DATA_PATH)\n",
        "print(\"Length of test dataset:\",test_data.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 103.30it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 125.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of training dataset: (100, 224, 224, 3)\n",
            "Length of test dataset: (10, 224, 224, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDmZDfHynLdi"
      },
      "source": [
        "## Construct model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DjjwFZ2nMAP"
      },
      "source": [
        "def encoder_decoder_model():\n",
        "\n",
        "    \"\"\"\n",
        "    Used to build Convolutional Autoencoder model architecture to get compressed image data which is easier to process.\n",
        "    Returns:\n",
        "    Auto encoder model\n",
        "    \"\"\"\n",
        "\n",
        "    #Encoder \n",
        "    model = Sequential(name='Convolutional_AutoEncoder_Model')\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu',input_shape=(224, 224, 3),padding='same', name='Encoding_Conv2D_1'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', name='Encoding_MaxPooling2D_1'))\n",
        "    model.add(Conv2D(128, kernel_size=(3, 3),strides=1,kernel_regularizer = tf.keras.regularizers.L2(0.001),activation='relu',padding='same', name='Encoding_Conv2D_2'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', name='Encoding_MaxPooling2D_2'))\n",
        "    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu',kernel_regularizer= tf.keras.regularizers.L2(0.001), padding='same', name='Encoding_Conv2D_3'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', name='Encoding_MaxPooling2D_3'))\n",
        "    model.add(Conv2D(512, kernel_size=(3, 3), activation='relu',kernel_regularizer= tf.keras.regularizers.L2(0.001), padding='same', name='Encoding_Conv2D_4'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2,padding='valid', name='Encoding_MaxPooling2D_4'))\n",
        "    model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', name='Encoding_Conv2D_5'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "    #Decoder\n",
        "    model.add(Conv2D(512, kernel_size=(3, 3), kernel_regularizer = tf.keras.regularizers.L2(0.001),activation='relu', padding='same', name='Decoding_Conv2D_1'))\n",
        "    model.add(UpSampling2D((2, 2), name='Decoding_Upsamping2D_1'))\n",
        "    model.add(Conv2D(512, kernel_size=(3, 3), kernel_regularizer = tf.keras.regularizers.L2(0.001), activation='relu', padding='same', name='Decoding_Conv2D_2'))\n",
        "    model.add(UpSampling2D((2, 2), name='Decoding_Upsamping2D_2'))\n",
        "    model.add(Conv2D(256, kernel_size=(3, 3), kernel_regularizer = tf.keras.regularizers.L2(0.001), activation='relu', padding='same',name='Decoding_Conv2D_3'))\n",
        "    model.add(UpSampling2D((2, 2),name='Decoding_Upsamping2D_3'))\n",
        "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_regularizer = tf.keras.regularizers.L2(0.001), padding='same',name='Decoding_Conv2D_4'))\n",
        "    model.add(UpSampling2D((2, 2),name='Decoding_Upsamping2D_4'))\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer = tf.keras.regularizers.L2(0.001), padding='same',name='Decoding_Conv2D_5'))\n",
        "    model.add(UpSampling2D((2, 2),name='Decoding_Upsamping2D_5'))\n",
        "    model.add(Conv2D(3, kernel_size=(3, 3), padding='same',activation='sigmoid',name='Decoding_Output'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCAqUkNqnbg4",
        "outputId": "ac0ae5d1-6f0f-4bc5-d393-94cefc8ee3b1"
      },
      "source": [
        "model = encoder_decoder_model()\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Convolutional_AutoEncoder_Model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Encoding_Conv2D_1 (Conv2D)   (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "Encoding_MaxPooling2D_1 (Max (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "Encoding_Conv2D_2 (Conv2D)   (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "Encoding_MaxPooling2D_2 (Max (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "Encoding_Conv2D_3 (Conv2D)   (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "Encoding_MaxPooling2D_3 (Max (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "Encoding_Conv2D_4 (Conv2D)   (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "Encoding_MaxPooling2D_4 (Max (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "Encoding_Conv2D_5 (Conv2D)   (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "Decoding_Conv2D_1 (Conv2D)   (None, 7, 7, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "Decoding_Upsamping2D_1 (UpSa (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "Decoding_Conv2D_2 (Conv2D)   (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "Decoding_Upsamping2D_2 (UpSa (None, 28, 28, 512)       0         \n",
            "_________________________________________________________________\n",
            "Decoding_Conv2D_3 (Conv2D)   (None, 28, 28, 256)       1179904   \n",
            "_________________________________________________________________\n",
            "Decoding_Upsamping2D_3 (UpSa (None, 56, 56, 256)       0         \n",
            "_________________________________________________________________\n",
            "Decoding_Conv2D_4 (Conv2D)   (None, 56, 56, 128)       295040    \n",
            "_________________________________________________________________\n",
            "Decoding_Upsamping2D_4 (UpSa (None, 112, 112, 128)     0         \n",
            "_________________________________________________________________\n",
            "Decoding_Conv2D_5 (Conv2D)   (None, 112, 112, 64)      73792     \n",
            "_________________________________________________________________\n",
            "Decoding_Upsamping2D_5 (UpSa (None, 224, 224, 64)      0         \n",
            "_________________________________________________________________\n",
            "Decoding_Output (Conv2D)     (None, 224, 224, 3)       1731      \n",
            "=================================================================\n",
            "Total params: 10,180,867\n",
            "Trainable params: 10,180,867\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW9yai_JnedK"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7rXn304nfgN",
        "outputId": "d82c5b7b-d049-4356-a119-e9abf80850c4"
      },
      "source": [
        "optimizer = Adam(learning_rate=0.001) \n",
        "model = encoder_decoder_model() \n",
        "model.compile(optimizer=optimizer, loss='mse') \n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min',verbose=1,patience=6,min_delta=0.0001) \n",
        "checkpoint = ModelCheckpoint('encoder_model.h5', monitor='val_loss', mode='min', save_best_only=True) \n",
        "model.fit(train_data, train_data, epochs=35, batch_size=32,validation_data=(test_data,test_data),callbacks=[early_stopping,checkpoint])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "4/4 [==============================] - 98s 22s/step - loss: 2.0691 - val_loss: 1.5625\n",
            "Epoch 2/35\n",
            "4/4 [==============================] - 93s 21s/step - loss: 1.4150 - val_loss: 1.0375\n",
            "Epoch 3/35\n",
            "4/4 [==============================] - 93s 21s/step - loss: 0.9330 - val_loss: 0.6686\n",
            "Epoch 4/35\n",
            "4/4 [==============================] - 92s 21s/step - loss: 0.5990 - val_loss: 0.4309\n",
            "Epoch 5/35\n",
            "4/4 [==============================] - 93s 21s/step - loss: 0.3850 - val_loss: 0.2782\n",
            "Epoch 6/35\n",
            "4/4 [==============================] - 95s 22s/step - loss: 0.2562 - val_loss: 0.1926\n",
            "Epoch 7/35\n",
            "4/4 [==============================] - 93s 21s/step - loss: 0.1827 - val_loss: 0.1456\n",
            "Epoch 8/35\n",
            "4/4 [==============================] - 93s 21s/step - loss: 0.1416 - val_loss: 0.1186\n",
            "Epoch 9/35\n",
            "4/4 [==============================] - 93s 21s/step - loss: 0.1179 - val_loss: 0.1015\n",
            "Epoch 10/35\n",
            "4/4 [==============================] - 93s 21s/step - loss: 0.1021 - val_loss: 0.0905\n",
            "Epoch 11/35\n",
            "4/4 [==============================] - 93s 21s/step - loss: 0.0906 - val_loss: 0.0795\n",
            "Epoch 12/35\n",
            "4/4 [==============================] - 93s 21s/step - loss: 0.0811 - val_loss: 0.0716\n",
            "Epoch 13/35\n",
            "4/4 [==============================] - 96s 21s/step - loss: 0.0735 - val_loss: 0.0662\n",
            "Epoch 14/35\n",
            "4/4 [==============================] - 93s 21s/step - loss: 0.0678 - val_loss: 0.0613\n",
            "Epoch 15/35\n",
            "4/4 [==============================] - 94s 22s/step - loss: 0.0636 - val_loss: 0.0583\n",
            "Epoch 16/35\n",
            "4/4 [==============================] - 95s 22s/step - loss: 0.0608 - val_loss: 0.0564\n",
            "Epoch 17/35\n",
            "4/4 [==============================] - 94s 21s/step - loss: 0.0589 - val_loss: 0.0544\n",
            "Epoch 18/35\n",
            "4/4 [==============================] - 93s 21s/step - loss: 0.0577 - val_loss: 0.0534\n",
            "Epoch 19/35\n",
            "4/4 [==============================] - 97s 23s/step - loss: 0.0567 - val_loss: 0.0524\n",
            "Epoch 20/35\n",
            "4/4 [==============================] - 94s 21s/step - loss: 0.0559 - val_loss: 0.0523\n",
            "Epoch 21/35\n",
            "4/4 [==============================] - 94s 21s/step - loss: 0.0552 - val_loss: 0.0511\n",
            "Epoch 22/35\n",
            "4/4 [==============================] - 94s 22s/step - loss: 0.0549 - val_loss: 0.0505\n",
            "Epoch 23/35\n",
            "4/4 [==============================] - 94s 22s/step - loss: 0.0545 - val_loss: 0.0503\n",
            "Epoch 24/35\n",
            "4/4 [==============================] - 94s 21s/step - loss: 0.0542 - val_loss: 0.0501\n",
            "Epoch 25/35\n",
            "4/4 [==============================] - 97s 23s/step - loss: 0.0540 - val_loss: 0.0510\n",
            "Epoch 26/35\n",
            "4/4 [==============================] - 94s 22s/step - loss: 0.0542 - val_loss: 0.0510\n",
            "Epoch 27/35\n",
            "4/4 [==============================] - 94s 21s/step - loss: 0.0536 - val_loss: 0.0505\n",
            "Epoch 28/35\n",
            "4/4 [==============================] - 94s 22s/step - loss: 0.0535 - val_loss: 0.0501\n",
            "Epoch 29/35\n",
            "4/4 [==============================] - 94s 21s/step - loss: 0.0535 - val_loss: 0.0502\n",
            "Epoch 30/35\n",
            "4/4 [==============================] - 94s 22s/step - loss: 0.0534 - val_loss: 0.0507\n",
            "Epoch 00030: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1f8b8b81d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jn6Ur7ZFCxu"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def feature_extraction(model, data, layer = 14):\n",
        "\n",
        "    \"\"\"\n",
        "    Creating a function to run the initial layers of the encoder model. (to get feature extraction from any layer of the model)\n",
        "    Arguments:\n",
        "    model - (Auto encoder model) - Trained model\n",
        "    data - (np.ndarray) - list of images to get feature extraction from trained model\n",
        "    layer - (int) - from which layer to take the features(by default = 4)\n",
        "    Returns:\n",
        "    pooled_array - (np.ndarray) - array of extracted features of given images\n",
        "    \"\"\"\n",
        "\n",
        "    encoded = K.function([model.layers[0].input],[model.layers[layer].output])\n",
        "    encoded_array = encoded([data])[0]\n",
        "    pooled_array = encoded_array.max(axis=-1)\n",
        "    return encoded_array\n",
        "\n",
        "encoded = feature_extraction(model, train_data[:10], 9)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XisUmJDHK-p",
        "outputId": "fa4e1058-4a8b-4a96-ec3e-7aa1a62606e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoded.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 7, 7, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6vHQWNKIgB-"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vse9WelsIgN2"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zbJXUnDG3DM",
        "outputId": "ac6b845a-c8ce-4a0e-9552-088def01e9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "# dimensionality reduction for visualization\n",
        "transform = TSNE \n",
        "trans = transform(n_components=2) \n",
        "\n",
        "X_encoded_reshape = encoded.reshape((10, 25088))\n",
        "\n",
        "\n",
        "values = trans.fit_transform(X_encoded_reshape) \n",
        "\n",
        "K = [4,5,6,7] #hyper parameter tuning\n",
        "for k in K:\n",
        "    print(\"if Number of clusters: \"+str(k))\n",
        "    #clustering the data\n",
        "    kmeans = KMeans(n_clusters = k, random_state=0).fit(X_encoded_reshape)\n",
        "    labels=kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    plt.figure(figsize=(10,5)) \n",
        "    plt.subplot(1,1,1)\n",
        "    plt.scatter(values[:,0], values[:,1], c= kmeans.labels_.astype(float), s=50, alpha=0.5)\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1], c=None, s=50)\n",
        "    plt.show()\n",
        "    for row in range(k): \n",
        "        iter=0\n",
        "        plt.figure(figsize=(13,3))\n",
        "        for i,iterator in enumerate(labels):\n",
        "            if iterator == row:\n",
        "                # img = cv2.imread(DATA_PATH+lisp[i])\n",
        "                img = cv2.imread(DATA_PATH + \"/1.jpg\")\n",
        "                img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "                plot_(img, \"\", \"\", 1, 6, iter+1, \"cluster=\"+str(row),\"\",\"\",\"\",True)\n",
        "                iter+=1\n",
        "            if iter>=5: break\n",
        "        plt.show()\n",
        "    print() "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "if Number of clusters: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-590dd1b8f5b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/1.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mplot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cluster=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0miter\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 936x216 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}