{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOlVc8bqo/KS6A0QMPQV2vu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjahan/semantic_similarity/blob/main/examples/modern_embeddings_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Nomic Embed: A Truly Open Embedding Model\n",
        "\n",
        "`Longer context 8192`\n",
        "\n",
        "https://www.nomic.ai/blog/posts/nomic-embed-text-v1\n",
        "\n",
        "`text embedding model with a 8192 context-length that outperforms OpenAI Ada-002 and text-embedding-3-small on both short and long context tasks.`\n",
        "\n",
        "https://huggingface.co/nomic-ai/nomic-embed-text-v1\n",
        "\n",
        "https://huggingface.co/nomic-ai/nomic-bert-2048\n",
        "\n",
        "https://www.databricks.com/blog/mosaicbert\n",
        "\n",
        "\n",
        "### Task instruction prefixes\n",
        "`search_document`\n",
        "\n",
        "`Purpose: embed texts as documents from a dataset`\n",
        "\n",
        "`This prefix is used for embedding texts as documents, for example as documents for a RAG index.`"
      ],
      "metadata": {
        "id": "WRRybk6-h4Vp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAYWONTdh1gZ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use clustering prefix\n",
        "\n",
        "### Purpose: embed texts to group them into clusters\n",
        "\n",
        "`This prefix is used for embedding texts in order to group them into clusters, discover common topics, or remove semantic duplicates.`\n",
        "\n",
        "https://sbert.net/"
      ],
      "metadata": {
        "id": "fX6nLRJKjYgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The sentences to encode\n",
        "sentences = [\n",
        "    \"clustering: The weather is lovely today.\",\n",
        "    \"clustering: It's so sunny outside!\",\n",
        "    \"clustering: He drove to the stadium.\",\n",
        "]\n",
        "\n",
        "# sentences = ['search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten']\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "print(embeddings.shape)\n",
        "# [3, 384]\n",
        "\n",
        "# 3. Calculate the embedding similarities\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "print(similarities)\n",
        "\n",
        "# SBERT results\n",
        "\n",
        "# tensor([[1.0000, 0.6660, 0.1046],\n",
        "#         [0.6660, 1.0000, 0.1411],\n",
        "#         [0.1046, 0.1411, 1.0000]])"
      ],
      "metadata": {
        "id": "nUSyTwo2iuGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E5-mistral-7b-instruct\n",
        "\n",
        "`Improving Text Embeddings with Large Language Models. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, arXiv 2024`\n",
        "\n",
        "`This model has 32 layers and the embedding size is 4096.`\n",
        "\n",
        "https://huggingface.co/intfloat/e5-mistral-7b-instruct\n",
        "\n",
        "### It crashes the notebook due to the model size!"
      ],
      "metadata": {
        "id": "FKYqjixfjsYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"intfloat/e5-mistral-7b-instruct\")\n",
        "# In case you want to reduce the maximum sequence length:\n",
        "model.max_seq_length = 4096"
      ],
      "metadata": {
        "id": "JdgY-kH6j2xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tresting\n",
        "\n",
        "`Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.`\n",
        "\n",
        "`Have a look at config_sentence_transformers.json for the prompts that are pre-configured, such as web_search_query, sts_query, and summarization_query. Additionally, check out unilm/e5/utils.py for prompts we used for evaluation. You can use these via e.g. model.encode(queries, prompt=\"Instruct: Given a claim, find documents that refute the claim\\nQuery: \").`"
      ],
      "metadata": {
        "id": "XI1i1NcPj-QT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"how much protein should a female eat\",\n",
        "    \"summit define\",\n",
        "]\n",
        "documents = [\n",
        "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
        "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n",
        "]\n",
        "\n",
        "query_embeddings = model.encode(queries, prompt_name=\"web_search_query\")\n",
        "document_embeddings = model.encode(documents)\n",
        "\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ],
      "metadata": {
        "id": "P-vpri0ti1I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## jina-embeddings-v2-base-en\n",
        "\n",
        "`jina-embeddings-v2-base-en is an English, monolingual embedding model supporting 8192 sequence length. It is based on a BERT architecture (JinaBERT) that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length. The backbone jina-bert-v2-base-en is pretrained on the C4 dataset. The model is further trained on Jina AI's collection of more than 400 millions of sentence pairs and hard negatives. These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.`\n",
        "\n",
        "`The embedding model was trained using 512 sequence length, but extrapolates to 8k sequence length (or even longer) thanks to ALiBi. This makes our model useful for a range of use cases, especially when processing long documents is needed, including long document retrieval, semantic textual similarity, text reranking, recommendation, RAG and LLM-based generative search, etc.`\n",
        "\n",
        "`With a standard size of 137 million parameters, the model enables fast inference while delivering better performance than our small model. It is recommended to use a single GPU for inference.`\n",
        "\n",
        "https://huggingface.co/jinaai/jina-embeddings-v2-base-en"
      ],
      "metadata": {
        "id": "DmTpvz-UlGUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers\n",
        "from transformers import AutoModel\n",
        "from numpy.linalg import norm\n",
        "\n",
        "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
        "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True) # trust_remote_code is needed to use the encode method\n"
      ],
      "metadata": {
        "id": "52K88wHbkmhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.encode(['How is the weather today?', 'What is the current weather like today?'])\n",
        "print(cos_sim(embeddings[0], embeddings[1]))\n"
      ],
      "metadata": {
        "id": "J7ZqAhumkoHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\n",
        "\n",
        "`Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.`\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/2108.12409\n"
      ],
      "metadata": {
        "id": "bcu7lg8nlwI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ModernBERT\n",
        "\n",
        "`Finally, a Replacement for BERT`\n",
        "\n",
        "`This blog post introduces ModernBERT, a family of state-of-the-art encoder-only models representing improvements over older generation encoders across the board, with a 8192 sequence length, better downstream performance and much faster processing.`\n",
        "\n",
        "`ModernBERT is available as a slot-in replacement for any BERT-like models, with both a base (149M params) and large (395M params) model size.`\n",
        "\n",
        "https://huggingface.co/blog/modernbert\n",
        "\n",
        "## gte-modernbert-base\n",
        "\n",
        "`We are excited to introduce the gte-modernbert series of models, which are built upon the latest modernBERT pre-trained encoder-only foundation models. The gte-modernbert series models include both text embedding models and rerank models.`\n",
        "\n",
        "`The gte-modernbert models demonstrates competitive performance in several text embedding and text retrieval evaluation tasks when compared to similar-scale models from the current open-source community. This includes assessments such as MTEB, LoCO, and COIR evaluation.`\n",
        "\n",
        "https://huggingface.co/Alibaba-NLP/gte-modernbert-base"
      ],
      "metadata": {
        "id": "9XY19WaUmG1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Requires transformers>=4.48.0\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_path = \"Alibaba-NLP/gte-modernbert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModel.from_pretrained(model_path)\n",
        "\n",
        "# Tokenize the input texts\n",
        "batch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')"
      ],
      "metadata": {
        "id": "JH2BK5Aul1s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = [\n",
        "    \"what is the capital of China?\",\n",
        "    \"how to implement quick sort in python?\",\n",
        "    \"Beijing\",\n",
        "    \"sorting algorithms\"\n",
        "]\n",
        "\n",
        "outputs = model(**batch_dict)\n",
        "embeddings = outputs.last_hidden_state[:, 0]\n",
        "\n",
        "# (Optionally) normalize embeddings\n",
        "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "scores = (embeddings[:1] @ embeddings[1:].T) * 100\n",
        "print(scores.tolist())\n",
        "# [[42.89073944091797, 71.30911254882812, 33.664554595947266]]"
      ],
      "metadata": {
        "id": "OWbg5uiAnhU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optionally) normalize embeddings\n",
        "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "scores = (embeddings[1:2] @ embeddings[2:].T) * 100\n",
        "print(scores.tolist())\n",
        "# [[42.89073944091797, 71.30911254882812, 33.664554595947266]]"
      ],
      "metadata": {
        "id": "fxxh-BaBnni9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Requires transformers>=4.48.0\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.util import cos_sim\n",
        "\n",
        "input_texts = [\n",
        "    \"what is the capital of China?\",\n",
        "    \"how to implement quick sort in python?\",\n",
        "    \"Beijing\",\n",
        "    \"sorting algorithms\"\n",
        "]\n",
        "\n",
        "model = SentenceTransformer(\"Alibaba-NLP/gte-modernbert-base\")\n",
        "embeddings = model.encode(input_texts)\n",
        "print(embeddings.shape)\n",
        "# (4, 768)\n",
        "\n",
        "similarities = cos_sim(embeddings[0], embeddings[1:])\n",
        "print(similarities)\n",
        "# tensor([[0.4289, 0.7131, 0.3366]])"
      ],
      "metadata": {
        "id": "a9XDs-Lmn5Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gte-reranker-modernbert-base\n",
        "\n",
        "`We are excited to introduce the gte-modernbert series of models, which are built upon the latest modernBERT pre-trained encoder-only foundation models. The gte-modernbert series models include both text embedding models and rerank models.`\n",
        "\n",
        "`The gte-modernbert models demonstrates competitive performance in several text embedding and text retrieval evaluation tasks when compared to similar-scale models from the current open-source community. This includes assessments such as MTEB, LoCO, and COIR evaluation.`"
      ],
      "metadata": {
        "id": "GS3xwtRrpFIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Requires transformers>=4.48.0\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name_or_path = \"Alibaba-NLP/gte-reranker-modernbert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "model.eval()\n",
        "\n"
      ],
      "metadata": {
        "id": "6Yh--jWwpFc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [\n",
        "    [\"what is the capital of China?\", \"Beijing\"],\n",
        "    [\"how to implement quick sort in python?\", \"Introduction of quick sort\"],\n",
        "    [\"how to implement quick sort in python?\", \"The weather is nice today\"],\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
        "    print(scores)\n",
        "\n",
        "# tensor([ 2.1387,  2.4609, -1.6729])\n"
      ],
      "metadata": {
        "id": "tBbJJ9iCpHTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Requires transformers>=4.48.0\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "model = CrossEncoder(\n",
        "    \"Alibaba-NLP/gte-reranker-modernbert-base\",\n",
        "    automodel_args={\"torch_dtype\": \"auto\"},\n",
        ")\n",
        "\n",
        "pairs = [\n",
        "    [\"what is the capital of China?\", \"Beijing\"],\n",
        "    [\"how to implement quick sort in python?\",\"Introduction of quick sort\"],\n",
        "    [\"how to implement quick sort in python?\", \"The weather is nice today\"],\n",
        "]\n",
        "\n",
        "scores = model.predict(pairs)\n",
        "print(scores)\n",
        "# [0.8945664  0.9213594  0.15742092]\n",
        "# NOTE: Sentence Transformers calls Softmax over the outputs by default, hence the scores are in [0, 1] range.\n"
      ],
      "metadata": {
        "id": "bLQFE8YopP8n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}